{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T18:05:04.941847Z",
     "iopub.status.busy": "2025-05-16T18:05:04.866747Z",
     "iopub.status.idle": "2025-05-16T18:46:50.835139Z",
     "shell.execute_reply": "2025-05-16T18:46:50.835610Z",
     "shell.execute_reply.started": "2025-04-28T03:24:43.490506Z"
    },
    "papermill": {
     "duration": 2505.99362,
     "end_time": "2025-05-16T18:46:50.835848",
     "exception": false,
     "start_time": "2025-05-16T18:05:04.842228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 1.6.0\n",
      "CUDA available: True\n",
      "CUDA device: Tesla P100-PCIE-16GB\n",
      "Initializing model...\n",
      "Setting up loss functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20d6e79296440ed920c9d621bf843f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Training from scratch *** \n",
      "train loss: 0.05567 | step: 250/2000 | lr: 0.0002000 | time_used: 4.0\n",
      "step: 250 | ssim: 0.8093 | psnr: 24.4913\n",
      "\n",
      " model saved at step : 250 | max_psnr: 24.4913 | max_ssim: 0.8093\n",
      "train loss: 0.05561 | step: 500/2000 | lr: 0.0002000 | time_used: 9.2\n",
      "step: 500 | ssim: 0.8639 | psnr: 24.4138\n",
      "train loss: 0.04892 | step: 750/2000 | lr: 0.0002000 | time_used: 14.4\n",
      "step: 750 | ssim: 0.8843 | psnr: 22.5842\n",
      "train loss: 0.04777 | step: 1000/2000 | lr: 0.0002000 | time_used: 19.6\n",
      "step: 1000 | ssim: 0.9033 | psnr: 24.9370\n",
      "\n",
      " model saved at step : 1000 | max_psnr: 24.9370 | max_ssim: 0.9033\n",
      "train loss: 0.05641 | step: 1250/2000 | lr: 0.0002000 | time_used: 24.8\n",
      "step: 1250 | ssim: 0.9068 | psnr: 23.2187\n",
      "train loss: 0.04198 | step: 1500/2000 | lr: 0.0002000 | time_used: 29.9\n",
      "step: 1500 | ssim: 0.9177 | psnr: 25.3192\n",
      "\n",
      " model saved at step : 1500 | max_psnr: 25.3192 | max_ssim: 0.9177\n",
      "train loss: 0.02861 | step: 1750/2000 | lr: 0.0002000 | time_used: 35.1\n",
      "step: 1750 | ssim: 0.9373 | psnr: 27.8527\n",
      "\n",
      " model saved at step : 1750 | max_psnr: 27.8527 | max_ssim: 0.9373\n",
      "train loss: 0.05391 | step: 2000/2000 | lr: 0.0002000 | time_used: 40.3\n",
      "step: 2000 | ssim: 0.9360 | psnr: 26.8076\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "import time, math\n",
    "import argparse, random\n",
    "from math import exp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.transforms import functional as FF\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "steps = 2000  \n",
    "resume = False\n",
    "eval_step = 250  \n",
    "learning_rate = 0.0002\n",
    "pretrained_model_dir = './pretrained_models/'\n",
    "model_dir = './trained_models/'\n",
    "trainset = 'its_train'\n",
    "testset = 'its_test'\n",
    "network = 'ffa'\n",
    "gps = 3\n",
    "blocks = 12  \n",
    "bs = 4  \n",
    "crop = True\n",
    "crop_size = 240  \n",
    "no_lr_sche = True\n",
    "perloss = True\n",
    "\n",
    "model_name = trainset + '_' + network.split('.')[0] + '_' + str(gps) + '_' + str(blocks)\n",
    "pretrained_model_dir = pretrained_model_dir + model_name + '.pk'\n",
    "model_dir = model_dir + model_name + '.pk'\n",
    "log_dir = 'logs/' + model_name\n",
    "\n",
    "necessary_dirs = ['trained_models', 'numpy_files', 'logs', 'samples', f'samples/{model_name}', 'pretrained_models']\n",
    "for dir_path in necessary_dirs:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "crop_size='whole_img' if not crop else crop_size\n",
    "\n",
    "def tensorShow(tensors, titles=None):\n",
    "    '''t:BCWH'''\n",
    "    fig=plt.figure()\n",
    "    for tensor, title, i in zip(tensors, titles, range(len(tensors))):\n",
    "        img = make_grid(tensor)\n",
    "        npimg = img.numpy()\n",
    "        ax = fig.add_subplot(211+i)\n",
    "        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        ax.set_title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def lr_schedule_cosdecay(t, T, init_lr=learning_rate):\n",
    "    lr=0.5*(1+math.cos(t*math.pi/T))*init_lr\n",
    "    return lr\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "    \n",
    "class PALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(PALayer, self).__init__()\n",
    "        self.pa = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.pa(x)\n",
    "        return x * y\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CALayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.ca = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, conv, dim, kernel_size,):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n",
    "        self.calayer = CALayer(dim)\n",
    "        self.palayer = PALayer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.act1(self.conv1(x))\n",
    "        res = res+x \n",
    "        res = self.conv2(res)\n",
    "        res = self.calayer(res)\n",
    "        res = self.palayer(res)\n",
    "        res += x \n",
    "        return res\n",
    "\n",
    "class Group(nn.Module):\n",
    "    def __init__(self, conv, dim, kernel_size, blocks):\n",
    "        super(Group, self).__init__()\n",
    "        modules = [Block(conv, dim, kernel_size)  for _ in range(blocks)]\n",
    "        modules.append(conv(dim, dim, kernel_size))\n",
    "        self.gp = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.gp(x)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "class FFA(nn.Module):\n",
    "    def __init__(self, gps, blocks, conv=default_conv):\n",
    "        super(FFA, self).__init__()\n",
    "        self.gps = gps\n",
    "        self.dim = 64\n",
    "        kernel_size = 3\n",
    "        pre_process = [conv(3, self.dim, kernel_size)]\n",
    "        assert self.gps==3\n",
    "        self.g1 = Group(conv, self.dim, kernel_size, blocks=blocks)\n",
    "        self.g2 = Group(conv, self.dim, kernel_size, blocks=blocks)\n",
    "        self.g3 = Group(conv, self.dim, kernel_size, blocks=blocks)\n",
    "        self.ca = nn.Sequential(*[\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(self.dim*self.gps, self.dim//16, 1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.dim//16, self.dim*self.gps, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "            ])\n",
    "        self.palayer = PALayer(self.dim)\n",
    "\n",
    "        post_process = [\n",
    "            conv(self.dim, self.dim, kernel_size),\n",
    "            conv(self.dim, 3, kernel_size)]\n",
    "\n",
    "        self.pre = nn.Sequential(*pre_process)\n",
    "        self.post = nn.Sequential(*post_process)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        x = self.pre(x1)\n",
    "        res1 = self.g1(x)\n",
    "        res2 = self.g2(res1)\n",
    "        res3 = self.g3(res2)\n",
    "        w = self.ca(torch.cat([res1, res2, res3], dim=1))\n",
    "        w = w.view(-1, self.gps, self.dim)[:, :, :, None, None]\n",
    "        out = w[:, 0, ::] * res1 + w[:, 1, ::] * res2 + w[:, 2, ::] * res3\n",
    "        out = self.palayer(out)\n",
    "        x = self.post(out)\n",
    "        return x + x1\n",
    "\n",
    "# --- Perceptual loss network  --- #\n",
    "class PerLoss(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(PerLoss, self).__init__()\n",
    "        self.vgg_layers = vgg_model\n",
    "        self.layer_name_mapping = {\n",
    "            '3': \"relu1_2\",\n",
    "            '8': \"relu2_2\",\n",
    "            '15': \"relu3_3\"\n",
    "        }\n",
    "\n",
    "    def output_features(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.vgg_layers._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return list(output.values())\n",
    "\n",
    "    def forward(self, dehaze, gt):\n",
    "        loss = []\n",
    "        dehaze_features = self.output_features(dehaze)\n",
    "        gt_features = self.output_features(gt)\n",
    "        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n",
    "            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n",
    "\n",
    "        return sum(loss)/len(loss)\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "    img1 = torch.clamp(img1, min=0, max=1)\n",
    "    img2 = torch.clamp(img2, min=0, max=1)\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "def psnr(pred, gt):\n",
    "    pred = pred.clamp(0, 1).cpu().numpy()\n",
    "    gt = gt.clamp(0, 1).cpu().numpy()\n",
    "    imdff = pred - gt\n",
    "    rmse = math.sqrt(np.mean(imdff ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    return 20 * math.log10(1.0 / rmse)\n",
    "\n",
    "class BronchoScopy_Dataset(data.Dataset):\n",
    "    def __init__(self, haze_list, clear_list, train=True, size=crop_size):\n",
    "        super(BronchoScopy_Dataset, self).__init__()\n",
    "        self.haze_imgs = haze_list\n",
    "        self.clear_imgs = clear_list\n",
    "        self.size = size\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        haze = Image.open(self.haze_imgs[index]).convert('RGB')\n",
    "        clear = Image.open(self.clear_imgs[index]).convert('RGB')\n",
    "\n",
    "        if isinstance(self.size, int):\n",
    "            while haze.size[0] < self.size or haze.size[1] < self.size:\n",
    "                index = random.randint(0, len(self.haze_imgs) - 1)\n",
    "                haze = Image.open(self.haze_imgs[index]).convert('RGB')\n",
    "                clear = Image.open(self.clear_imgs[index]).convert('RGB')\n",
    "\n",
    "        clear = tfs.CenterCrop(haze.size[::-1])(clear)\n",
    "        \n",
    "        if not isinstance(self.size, str):\n",
    "            i, j, h, w = tfs.RandomCrop.get_params(haze, output_size=(self.size, self.size))\n",
    "            haze = FF.crop(haze, i, j, h, w)\n",
    "            clear = FF.crop(clear, i, j, h, w)\n",
    "\n",
    "        haze, clear = self.augData(haze, clear)\n",
    "        return haze, clear\n",
    "\n",
    "    def augData(self, data, target):\n",
    "        if self.train:\n",
    "            rand_hor = random.randint(0, 1)\n",
    "            rand_rot = random.randint(0, 3)\n",
    "            data = tfs.RandomHorizontalFlip(rand_hor)(data)\n",
    "            target = tfs.RandomHorizontalFlip(rand_hor)(target)\n",
    "            if rand_rot:\n",
    "                data = FF.rotate(data, 90 * rand_rot)\n",
    "                target = FF.rotate(target, 90 * rand_rot)\n",
    "        data = tfs.ToTensor()(data)\n",
    "        data = tfs.Normalize(mean=[0.64, 0.6, 0.58], std=[0.14, 0.15, 0.152])(data)\n",
    "        target = tfs.ToTensor()(target)\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_imgs)\n",
    "\n",
    "\n",
    "\n",
    "def train(net, loader_train, loader_test, optim, criterion, device):\n",
    "    losses = []\n",
    "    start_step = 0\n",
    "    max_ssim = max_psnr = 0\n",
    "    ssims, psnrs = [], []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if resume and os.path.exists(pretrained_model_dir):\n",
    "        print(f'resume from {pretrained_model_dir}')\n",
    "        ckp = torch.load(pretrained_model_dir, map_location=device)\n",
    "        losses = ckp['losses']\n",
    "        net.load_state_dict(ckp['model'])\n",
    "        start_step = ckp['step']\n",
    "        max_ssim = ckp['max_ssim']\n",
    "        max_psnr = ckp['max_psnr']\n",
    "        psnrs = ckp['psnrs']\n",
    "        ssims = ckp['ssims']\n",
    "        print(f'Resuming training from step: {start_step} ***')\n",
    "    else:\n",
    "        print('Training from scratch *** ')\n",
    "        \n",
    "    for step in range(start_step+1, steps+1):\n",
    "        net.train()\n",
    "        lr = learning_rate\n",
    "        if not no_lr_sche:\n",
    "            lr = lr_schedule_cosdecay(step, steps)\n",
    "            for param_group in optim.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "                \n",
    "        x, y = next(iter(loader_train))\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = net(x)\n",
    "        loss = criterion[0](out, y)\n",
    "        if perloss:\n",
    "            loss2 = criterion[1](out, y)\n",
    "            loss = loss + 0.04*loss2\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Clear CUDA cache periodically\n",
    "        if step % 10 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f'\\rtrain loss: {loss.item():.5f} | step: {step}/{steps} | lr: {lr :.7f} | time_used: {(time.time()-start_time)/60 :.1f}', end='', flush=True)\n",
    "\n",
    "        if step % eval_step == 0:\n",
    "            with torch.no_grad():\n",
    "                ssim_eval, psnr_eval = test(net, loader_test, max_psnr, max_ssim, step, device)\n",
    "            print(f'\\nstep: {step} | ssim: {ssim_eval:.4f} | psnr: {psnr_eval:.4f}')\n",
    "\n",
    "            ssims.append(ssim_eval)\n",
    "            psnrs.append(psnr_eval)\n",
    "            if ssim_eval > max_ssim and psnr_eval > max_psnr:\n",
    "                max_ssim = max(max_ssim, ssim_eval)\n",
    "                max_psnr = max(max_psnr, psnr_eval)\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'max_psnr': max_psnr,\n",
    "                    'max_ssim': max_ssim,\n",
    "                    'ssims': ssims,\n",
    "                    'psnrs': psnrs,\n",
    "                    'losses': losses,\n",
    "                    'model': net.state_dict()\n",
    "                }, model_dir)\n",
    "                print(f'\\n model saved at step : {step} | max_psnr: {max_psnr:.4f} | max_ssim: {max_ssim:.4f}')\n",
    "\n",
    "    np.save(f'./numpy_files/{model_name}_{steps}_losses.npy', losses)\n",
    "    np.save(f'./numpy_files/{model_name}_{steps}_ssims.npy', ssims)\n",
    "    np.save(f'./numpy_files/{model_name}_{steps}_psnrs.npy', psnrs)\n",
    "\n",
    "def test(net, loader_test, max_psnr, max_ssim, step, device):\n",
    "    net.eval()\n",
    "    # Clear CUDA cache before testing\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    ssims, psnrs = [], []\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(loader_test):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        pred = net(inputs)\n",
    "        \n",
    "        ssim1 = ssim(pred, targets).item()\n",
    "        psnr1 = psnr(pred, targets)\n",
    "        ssims.append(ssim1)\n",
    "        psnrs.append(psnr1)\n",
    "        \n",
    "        \n",
    "        if i < 2:  \n",
    "            ts = vutils.make_grid([torch.squeeze(inputs.cpu()), \n",
    "                                   torch.squeeze(targets.cpu()),\n",
    "                                   torch.squeeze(pred.clamp(0, 1).cpu())])\n",
    "            vutils.save_image(ts, f'samples/{model_name}/{step}_{i}_{psnr1:.4f}_{ssim1:.4f}.png')\n",
    "    \n",
    "    return np.mean(ssims), np.mean(psnrs)\n",
    "\n",
    "\n",
    "def main():\n",
    "    haze_dir = \"/kaggle/input/hazy-bronchoscopic-images/hazy_bronchoscopic_images/hazy_bronchoscopic_images\"\n",
    "    clear_dir = \"/kaggle/input/hazy-bronchoscopic-images/clear_bronchoscopic_images/clear_bronchoscopic_images\"\n",
    "\n",
    "    haze_images = sorted([os.path.join(haze_dir, x) for x in os.listdir(haze_dir) if x.endswith('.jpg') or x.endswith('.png')])\n",
    "    clear_images = sorted([os.path.join(clear_dir, x) for x in os.listdir(clear_dir) if x.endswith('.jpg') or x.endswith('.png')])\n",
    "\n",
    "    \n",
    "    assert len(haze_images) == len(clear_images), \"Mismatch between hazy and clear images!\"\n",
    "    \n",
    "    # Split data \n",
    "    haze_train, haze_temp, clear_train, clear_temp = train_test_split(haze_images, clear_images, test_size=0.2, random_state=42)\n",
    "    haze_val, haze_test, clear_val, clear_test = train_test_split(haze_temp, clear_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_dataset = BronchoScopy_Dataset(haze_train, clear_train, train=True, size=crop_size)\n",
    "    val_dataset = BronchoScopy_Dataset(haze_val, clear_val, train=False, size='whole_img')\n",
    "    test_dataset = BronchoScopy_Dataset(haze_test, clear_test, train=False, size='whole_img')\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    net = FFA(gps=gps, blocks=blocks)\n",
    "    net = net.to(device)\n",
    "\n",
    "    print(\"Setting up loss functions...\")\n",
    "    criterion = [nn.L1Loss().to(device)]\n",
    "    if perloss:\n",
    "        vgg_model = vgg16(pretrained=True).features[:16]\n",
    "        vgg_model = vgg_model.to(device)\n",
    "        for param in vgg_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        criterion.append(PerLoss(vgg_model).to(device))\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train(net, train_loader, val_loader, optimizer, criterion, device)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "# For inference\n",
    "def test_on_images(image_dir, output_dir, model_path):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    " \n",
    "    net = FFA(gps=gps, blocks=blocks)\n",
    "    net = net.to(device)\n",
    "    ckp = torch.load(model_path, map_location=device)\n",
    "    net.load_state_dict(ckp['model'])\n",
    "    net.eval()\n",
    "    \n",
    "    \n",
    "    for im in os.listdir(image_dir):\n",
    "        if not (im.lower().endswith('.jpg') or im.lower().endswith('.png')):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {im}...\")\n",
    "        haze = Image.open(os.path.join(image_dir, im))\n",
    "        \n",
    "        # Normalize \n",
    "        haze1 = tfs.Compose([\n",
    "            tfs.ToTensor(),\n",
    "            tfs.Normalize(mean=[0.64, 0.6, 0.58], std=[0.14, 0.15, 0.152])\n",
    "        ])(haze)[None, ::]\n",
    "        \n",
    "        haze_no = tfs.ToTensor()(haze)[None, ::]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            haze1 = haze1.to(device)\n",
    "            pred = net(haze1)\n",
    "        \n",
    "        ts = torch.squeeze(pred.clamp(0, 1).cpu())\n",
    "        \n",
    "        haze_no = make_grid(haze_no, nrow=1, normalize=True)\n",
    "        ts = make_grid(ts, nrow=1, normalize=True)\n",
    "        \n",
    "        image_grid = torch.cat((haze_no, ts), -1)\n",
    "        vutils.save_image(image_grid, os.path.join(output_dir, f\"{im.split('.')[0]}_dehazed.png\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 937211,
     "sourceId": 1587463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 937212,
     "sourceId": 1587465,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7258641,
     "sourceId": 11576981,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7270586,
     "sourceId": 11594287,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 46355741,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30021,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2510.617356,
   "end_time": "2025-05-16T18:46:51.507421",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-16T18:05:00.890065",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "8fc5b31d9b664449b9fed1c57569473d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "913234bb4f054878aae1dc192630b1a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_df75ae5fac154efbbad2e5f5f0d70cdf",
       "placeholder": "​",
       "style": "IPY_MODEL_d7a0b12f8bf245ec821f05f1d0c088b6",
       "value": " 528M/528M [00:01&lt;00:00, 298MB/s]"
      }
     },
     "a70c24d32998495d96982014eb189caf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b1e8ecedb3004d379792be63f0d33c5f",
       "max": 553433881.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8fc5b31d9b664449b9fed1c57569473d",
       "value": 553433881.0
      }
     },
     "b1e8ecedb3004d379792be63f0d33c5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c20d6e79296440ed920c9d621bf843f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a70c24d32998495d96982014eb189caf",
        "IPY_MODEL_913234bb4f054878aae1dc192630b1a4"
       ],
       "layout": "IPY_MODEL_d18666273140444a8ecd54cd7df69ebb"
      }
     },
     "d18666273140444a8ecd54cd7df69ebb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7a0b12f8bf245ec821f05f1d0c088b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "df75ae5fac154efbbad2e5f5f0d70cdf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
