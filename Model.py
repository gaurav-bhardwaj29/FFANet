{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"duration":25751.020388,"end_time":"2020-11-07T15:16:05.952699","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-11-07T08:06:54.932311","version":"2.1.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"16e6236d7c18452fbce723dd4d8a0905":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1aaac943aea64897a69c51ab9b6db16a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"224948e91b7e4a34ab06e0b5efe459a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_1aaac943aea64897a69c51ab9b6db16a","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80181e42b1f44351baa55d1bea106231","value":553433881}},"334d0952574d408790df496eb3cd3d88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16e6236d7c18452fbce723dd4d8a0905","placeholder":"â€‹","style":"IPY_MODEL_74636338d0fd4e5ea56bf3685cae48b9","value":" 528M/528M [1:45:47&lt;00:00, 87.2kB/s]"}},"73cb8ba83f0042eeb230312d1b7ca2f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74636338d0fd4e5ea56bf3685cae48b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80181e42b1f44351baa55d1bea106231":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"9a6720f31ffb43a3a32c1d7ed912c819":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_224948e91b7e4a34ab06e0b5efe459a5","IPY_MODEL_334d0952574d408790df496eb3cd3d88"],"layout":"IPY_MODEL_73cb8ba83f0042eeb230312d1b7ca2f6"}}},"version_major":2,"version_minor":0}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1587463,"sourceType":"datasetVersion","datasetId":937211},{"sourceId":1587465,"sourceType":"datasetVersion","datasetId":937212},{"sourceId":11576981,"sourceType":"datasetVersion","datasetId":7258641},{"sourceId":11594287,"sourceType":"datasetVersion","datasetId":7270586},{"sourceId":46355741,"sourceType":"kernelVersion"}],"dockerImageVersionId":30021,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gauravbhradwaj/ffa-net-for-single-image-dehazing-pytorch?scriptVersionId=239468644\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport os, sys\nimport time, math\nimport argparse, random\nfrom math import exp\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Initialize device globally - switched from MPS to CUDA for Kaggle\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\nimport torchvision\nimport torchvision.transforms as tfs\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as FF\nimport torchvision.utils as vutils\nfrom torchvision.utils import make_grid\nfrom torchvision.models import vgg16\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Adjusted parameters for Kaggle environment (increased from M2 values)\nsteps = 3000  # Increased to original value for Kaggle GPUs\nresume = False\neval_step = 500  # Increased to original value for Kaggle GPUs\nlearning_rate = 0.0001\npretrained_model_dir = './pretrained_models/'\nmodel_dir = './trained_models/'\ntrainset = 'its_train'\ntestset = 'its_test'\nnetwork = 'ffa'\ngps = 3\nblocks = 12  # Increased back to original 12 blocks for Kaggle GPUs\nbs = 4  # Increased batch size for Kaggle GPUs\ncrop = True\ncrop_size = 240  # Increased crop size for Kaggle GPUs\nno_lr_sche = True\nperloss = True\n\nmodel_name = trainset + '_' + network.split('.')[0] + '_' + str(gps) + '_' + str(blocks)\npretrained_model_dir = pretrained_model_dir + model_name + '.pk'\nmodel_dir = model_dir + model_name + '.pk'\nlog_dir = 'logs/' + model_name\n\n# Create necessary directories\nnecessary_dirs = ['trained_models', 'numpy_files', 'logs', 'samples', f'samples/{model_name}', 'pretrained_models']\nfor dir_path in necessary_dirs:\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\ncrop_size='whole_img' if not crop else crop_size\n\ndef tensorShow(tensors, titles=None):\n    '''t:BCWH'''\n    fig=plt.figure()\n    for tensor, title, i in zip(tensors, titles, range(len(tensors))):\n        img = make_grid(tensor)\n        npimg = img.numpy()\n        ax = fig.add_subplot(211+i)\n        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n        ax.set_title(title)\n    plt.show()\n    \ndef lr_schedule_cosdecay(t, T, init_lr=learning_rate):\n    lr=0.5*(1+math.cos(t*math.pi/T))*init_lr\n    return lr\n\ndef default_conv(in_channels, out_channels, kernel_size, bias=True):\n    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n    \nclass PALayer(nn.Module):\n    def __init__(self, channel):\n        super(PALayer, self).__init__()\n        self.pa = nn.Sequential(\n                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n    def forward(self, x):\n        y = self.pa(x)\n        return x * y\n\nclass CALayer(nn.Module):\n    def __init__(self, channel):\n        super(CALayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ca = nn.Sequential(\n                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.ca(y)\n        return x * y\n\nclass Block(nn.Module):\n    def __init__(self, conv, dim, kernel_size,):\n        super(Block, self).__init__()\n        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n        self.calayer = CALayer(dim)\n        self.palayer = PALayer(dim)\n\n    def forward(self, x):\n        res = self.act1(self.conv1(x))\n        res = res+x \n        res = self.conv2(res)\n        res = self.calayer(res)\n        res = self.palayer(res)\n        res += x \n        return res\n\nclass Group(nn.Module):\n    def __init__(self, conv, dim, kernel_size, blocks):\n        super(Group, self).__init__()\n        modules = [Block(conv, dim, kernel_size)  for _ in range(blocks)]\n        modules.append(conv(dim, dim, kernel_size))\n        self.gp = nn.Sequential(*modules)\n\n    def forward(self, x):\n        res = self.gp(x)\n        res += x\n        return res\n\nclass FFA(nn.Module):\n    def __init__(self, gps, blocks, conv=default_conv):\n        super(FFA, self).__init__()\n        self.gps = gps\n        self.dim = 64\n        kernel_size = 3\n        pre_process = [conv(3, self.dim, kernel_size)]\n        assert self.gps==3\n        self.g1 = Group(conv, self.dim, kernel_size, blocks=blocks)\n        self.g2 = Group(conv, self.dim, kernel_size, blocks=blocks)\n        self.g3 = Group(conv, self.dim, kernel_size, blocks=blocks)\n        self.ca = nn.Sequential(*[\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(self.dim*self.gps, self.dim//16, 1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(self.dim//16, self.dim*self.gps, 1, padding=0, bias=True),\n            nn.Sigmoid()\n            ])\n        self.palayer = PALayer(self.dim)\n\n        post_process = [\n            conv(self.dim, self.dim, kernel_size),\n            conv(self.dim, 3, kernel_size)]\n\n        self.pre = nn.Sequential(*pre_process)\n        self.post = nn.Sequential(*post_process)\n\n    def forward(self, x1):\n        x = self.pre(x1)\n        res1 = self.g1(x)\n        res2 = self.g2(res1)\n        res3 = self.g3(res2)\n        w = self.ca(torch.cat([res1, res2, res3], dim=1))\n        w = w.view(-1, self.gps, self.dim)[:, :, :, None, None]\n        out = w[:, 0, ::] * res1 + w[:, 1, ::] * res2 + w[:, 2, ::] * res3\n        out = self.palayer(out)\n        x = self.post(out)\n        return x + x1\n\n# --- Perceptual loss network  --- #\nclass PerLoss(torch.nn.Module):\n    def __init__(self, vgg_model):\n        super(PerLoss, self).__init__()\n        self.vgg_layers = vgg_model\n        self.layer_name_mapping = {\n            '3': \"relu1_2\",\n            '8': \"relu2_2\",\n            '15': \"relu3_3\"\n        }\n\n    def output_features(self, x):\n        output = {}\n        for name, module in self.vgg_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n        return list(output.values())\n\n    def forward(self, dehaze, gt):\n        loss = []\n        dehaze_features = self.output_features(dehaze)\n        gt_features = self.output_features(gt)\n        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n\n        return sum(loss)/len(loss)\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n    return gauss / gauss.sum()\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\ndef ssim(img1, img2, window_size=11, size_average=True):\n    img1 = torch.clamp(img1, min=0, max=1)\n    img2 = torch.clamp(img2, min=0, max=1)\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n    return _ssim(img1, img2, window, window_size, channel, size_average)\n\ndef psnr(pred, gt):\n    pred = pred.clamp(0, 1).cpu().numpy()\n    gt = gt.clamp(0, 1).cpu().numpy()\n    imdff = pred - gt\n    rmse = math.sqrt(np.mean(imdff ** 2))\n    if rmse == 0:\n        return 100\n    return 20 * math.log10(1.0 / rmse)\n\nclass BronchoScopy_Dataset(data.Dataset):\n    def __init__(self, haze_list, clear_list, train=True, size=crop_size):\n        super(BronchoScopy_Dataset, self).__init__()\n        self.haze_imgs = haze_list\n        self.clear_imgs = clear_list\n        self.size = size\n        self.train = train\n\n    def __getitem__(self, index):\n        haze = Image.open(self.haze_imgs[index]).convert('RGB')\n        clear = Image.open(self.clear_imgs[index]).convert('RGB')\n\n        if isinstance(self.size, int):\n            while haze.size[0] < self.size or haze.size[1] < self.size:\n                index = random.randint(0, len(self.haze_imgs) - 1)\n                haze = Image.open(self.haze_imgs[index]).convert('RGB')\n                clear = Image.open(self.clear_imgs[index]).convert('RGB')\n\n        clear = tfs.CenterCrop(haze.size[::-1])(clear)\n        \n        if not isinstance(self.size, str):\n            i, j, h, w = tfs.RandomCrop.get_params(haze, output_size=(self.size, self.size))\n            haze = FF.crop(haze, i, j, h, w)\n            clear = FF.crop(clear, i, j, h, w)\n\n        haze, clear = self.augData(haze, clear)\n        return haze, clear\n\n    def augData(self, data, target):\n        if self.train:\n            rand_hor = random.randint(0, 1)\n            rand_rot = random.randint(0, 3)\n            data = tfs.RandomHorizontalFlip(rand_hor)(data)\n            target = tfs.RandomHorizontalFlip(rand_hor)(target)\n            if rand_rot:\n                data = FF.rotate(data, 90 * rand_rot)\n                target = FF.rotate(target, 90 * rand_rot)\n        data = tfs.ToTensor()(data)\n        data = tfs.Normalize(mean=[0.64, 0.6, 0.58], std=[0.14, 0.15, 0.152])(data)\n        target = tfs.ToTensor()(target)\n        return data, target\n\n    def __len__(self):\n        return len(self.haze_imgs)\n\n\n# Update the training and test functions to handle CUDA memory\ndef train(net, loader_train, loader_test, optim, criterion, device):\n    losses = []\n    start_step = 0\n    max_ssim = max_psnr = 0\n    ssims, psnrs = [], []\n    start_time = time.time()\n    \n    if resume and os.path.exists(pretrained_model_dir):\n        print(f'resume from {pretrained_model_dir}')\n        ckp = torch.load(pretrained_model_dir, map_location=device)\n        losses = ckp['losses']\n        net.load_state_dict(ckp['model'])\n        start_step = ckp['step']\n        max_ssim = ckp['max_ssim']\n        max_psnr = ckp['max_psnr']\n        psnrs = ckp['psnrs']\n        ssims = ckp['ssims']\n        print(f'Resuming training from step: {start_step} ***')\n    else:\n        print('Training from scratch *** ')\n        \n    for step in range(start_step+1, steps+1):\n        net.train()\n        lr = learning_rate\n        if not no_lr_sche:\n            lr = lr_schedule_cosdecay(step, steps)\n            for param_group in optim.param_groups:\n                param_group[\"lr\"] = lr\n                \n        x, y = next(iter(loader_train))\n        x = x.to(device)\n        y = y.to(device)\n        \n        # Forward pass\n        out = net(x)\n        loss = criterion[0](out, y)\n        if perloss:\n            loss2 = criterion[1](out, y)\n            loss = loss + 0.04*loss2\n\n        # Backward pass\n        loss.backward()\n        optim.step()\n        optim.zero_grad()\n        \n        losses.append(loss.item())\n        \n        # Clear CUDA cache periodically\n        if step % 10 == 0:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        \n        print(f'\\rtrain loss: {loss.item():.5f} | step: {step}/{steps} | lr: {lr :.7f} | time_used: {(time.time()-start_time)/60 :.1f}', end='', flush=True)\n\n        if step % eval_step == 0:\n            with torch.no_grad():\n                ssim_eval, psnr_eval = test(net, loader_test, max_psnr, max_ssim, step, device)\n            print(f'\\nstep: {step} | ssim: {ssim_eval:.4f} | psnr: {psnr_eval:.4f}')\n\n            ssims.append(ssim_eval)\n            psnrs.append(psnr_eval)\n            if ssim_eval > max_ssim and psnr_eval > max_psnr:\n                max_ssim = max(max_ssim, ssim_eval)\n                max_psnr = max(max_psnr, psnr_eval)\n                torch.save({\n                    'step': step,\n                    'max_psnr': max_psnr,\n                    'max_ssim': max_ssim,\n                    'ssims': ssims,\n                    'psnrs': psnrs,\n                    'losses': losses,\n                    'model': net.state_dict()\n                }, model_dir)\n                print(f'\\n model saved at step : {step} | max_psnr: {max_psnr:.4f} | max_ssim: {max_ssim:.4f}')\n\n    np.save(f'./numpy_files/{model_name}_{steps}_losses.npy', losses)\n    np.save(f'./numpy_files/{model_name}_{steps}_ssims.npy', ssims)\n    np.save(f'./numpy_files/{model_name}_{steps}_psnrs.npy', psnrs)\n\ndef test(net, loader_test, max_psnr, max_ssim, step, device):\n    net.eval()\n    # Clear CUDA cache before testing\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        \n    ssims, psnrs = [], []\n    \n    for i, (inputs, targets) in enumerate(loader_test):\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        \n        pred = net(inputs)\n        \n        ssim1 = ssim(pred, targets).item()\n        psnr1 = psnr(pred, targets)\n        ssims.append(ssim1)\n        psnrs.append(psnr1)\n        \n        # Optional: save sample images for visualization\n        if i < 2:  # Just save the first two samples to avoid too many images\n            ts = vutils.make_grid([torch.squeeze(inputs.cpu()), \n                                   torch.squeeze(targets.cpu()),\n                                   torch.squeeze(pred.clamp(0, 1).cpu())])\n            vutils.save_image(ts, f'samples/{model_name}/{step}_{i}_{psnr1:.4f}_{ssim1:.4f}.png')\n    \n    return np.mean(ssims), np.mean(psnrs)\n\n# Main execution function for Kaggle\ndef main():\n    # Modified for Kaggle paths\n    # In Kaggle, input data is usually in /kaggle/input/\n    # You'll need to adjust these paths based on your Kaggle dataset structure\n    haze_dir = \"/kaggle/input/hazy-bronchoscopic-images/hazy_bronchoscopic_images/hazy_bronchoscopic_images\"\n    clear_dir = \"/kaggle/input/hazy-bronchoscopic-images/clear_bronchoscopic_images/clear_bronchoscopic_images\"\n\n    haze_images = sorted([os.path.join(haze_dir, x) for x in os.listdir(haze_dir) if x.endswith('.jpg') or x.endswith('.png')])\n    clear_images = sorted([os.path.join(clear_dir, x) for x in os.listdir(clear_dir) if x.endswith('.jpg') or x.endswith('.png')])\n\n    # Ensure matching hazy and clear images\n    assert len(haze_images) == len(clear_images), \"Mismatch between hazy and clear images!\"\n    \n    # Split data into train/val/test sets\n    haze_train, haze_temp, clear_train, clear_temp = train_test_split(haze_images, clear_images, test_size=0.2, random_state=42)\n    haze_val, haze_test, clear_val, clear_test = train_test_split(haze_temp, clear_temp, test_size=0.5, random_state=42)\n\n    # Create datasets\n    train_dataset = BronchoScopy_Dataset(haze_train, clear_train, train=True, size=crop_size)\n    val_dataset = BronchoScopy_Dataset(haze_val, clear_val, train=False, size='whole_img')\n    test_dataset = BronchoScopy_Dataset(haze_test, clear_test, train=False, size='whole_img')\n\n    # Create data loaders\n    train_loader = DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)\n    test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n\n    print(\"Initializing model...\")\n    net = FFA(gps=gps, blocks=blocks)\n    net = net.to(device)\n\n    print(\"Setting up loss functions...\")\n    criterion = [nn.L1Loss().to(device)]\n    if perloss:\n        vgg_model = vgg16(pretrained=True).features[:16]\n        vgg_model = vgg_model.to(device)\n        for param in vgg_model.parameters():\n            param.requires_grad = False\n        criterion.append(PerLoss(vgg_model).to(device))\n\n    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n\n    print(\"Starting training...\")\n    train(net, train_loader, val_loader, optimizer, criterion, device)\n\n    print(\"Training completed!\")\n\n\n# For inference/testing on new images\ndef test_on_images(image_dir, output_dir, model_path):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Load model\n    net = FFA(gps=gps, blocks=blocks)\n    net = net.to(device)\n    ckp = torch.load(model_path, map_location=device)\n    net.load_state_dict(ckp['model'])\n    net.eval()\n    \n    # Process each image\n    for im in os.listdir(image_dir):\n        if not (im.lower().endswith('.jpg') or im.lower().endswith('.png')):\n            continue\n            \n        print(f\"Processing {im}...\")\n        haze = Image.open(os.path.join(image_dir, im))\n        \n        # Normalize and prepare for model\n        haze1 = tfs.Compose([\n            tfs.ToTensor(),\n            tfs.Normalize(mean=[0.64, 0.6, 0.58], std=[0.14, 0.15, 0.152])\n        ])(haze)[None, ::]\n        \n        haze_no = tfs.ToTensor()(haze)[None, ::]\n        \n        with torch.no_grad():\n            haze1 = haze1.to(device)\n            pred = net(haze1)\n        \n        # Convert to grid and save\n        ts = torch.squeeze(pred.clamp(0, 1).cpu())\n        \n        haze_no = make_grid(haze_no, nrow=1, normalize=True)\n        ts = make_grid(ts, nrow=1, normalize=True)\n        \n        # Create a side-by-side comparison\n        image_grid = torch.cat((haze_no, ts), -1)\n        vutils.save_image(image_grid, os.path.join(output_dir, f\"{im.split('.')[0]}_dehazed.png\"))\n\nif __name__ == \"__main__\":\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    \n    # Call main() to train the model\n    main()\n    \n    # For inference on test images after training:\n    # test_on_images('/kaggle/input/bronchoscopy-test-images', '/kaggle/working/results', model_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:24:43.490248Z","iopub.execute_input":"2025-04-28T03:24:43.490529Z","iopub.status.idle":"2025-04-28T04:19:33.531631Z","shell.execute_reply.started":"2025-04-28T03:24:43.490506Z","shell.execute_reply":"2025-04-28T04:19:33.530795Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nPyTorch version: 1.6.0\nCUDA available: True\nCUDA device: Tesla P100-PCIE-16GB\nInitializing model...\nSetting up loss functions...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481f3b51326b417ea1c6d7e6017647f2"}},"metadata":{}},{"name":"stdout","text":"\nStarting training...\nTraining from scratch *** \ntrain loss: 0.05730 | step: 500/3000 | lr: 0.0001000 | time_used: 7.9\nstep: 500 | ssim: 0.8459 | psnr: 25.5142\n\n model saved at step : 500 | max_psnr: 25.5142 | max_ssim: 0.8459\ntrain loss: 0.04407 | step: 1000/3000 | lr: 0.0001000 | time_used: 17.1\nstep: 1000 | ssim: 0.8900 | psnr: 25.9837\n\n model saved at step : 1000 | max_psnr: 25.9837 | max_ssim: 0.8900\ntrain loss: 0.04386 | step: 1500/3000 | lr: 0.0001000 | time_used: 26.1\nstep: 1500 | ssim: 0.9065 | psnr: 25.8333\ntrain loss: 0.04204 | step: 2000/3000 | lr: 0.0001000 | time_used: 35.2\nstep: 2000 | ssim: 0.9235 | psnr: 27.7283\n\n model saved at step : 2000 | max_psnr: 27.7283 | max_ssim: 0.9235\ntrain loss: 0.03591 | step: 2500/3000 | lr: 0.0001000 | time_used: 44.3\nstep: 2500 | ssim: 0.9289 | psnr: 27.3610\ntrain loss: 0.03339 | step: 3000/3000 | lr: 0.0001000 | time_used: 53.3\nstep: 3000 | ssim: 0.9412 | psnr: 28.5025\n\n model saved at step : 3000 | max_psnr: 28.5025 | max_ssim: 0.9412\nTraining completed!\n","output_type":"stream"}],"execution_count":8}]}